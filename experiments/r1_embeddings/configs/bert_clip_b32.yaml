# R1: Embedding Choice vs Accuracy/Latency
# BERT-base + CLIP ViT-B/32 Baseline

experiment:
  name: "r1_bert_clip_b32"
  description: "BERT-base text encoder + CLIP ViT-B/32 image encoder baseline"
  type: "r1_embeddings"
  
models:
  text_encoder:
    name: "bert-base-uncased"
    type: "bert"
    max_length: 512
    hidden_size: 768
    pooling: "mean"
    
  image_encoder:
    name: "ViT-B-32"
    type: "clip"
    pretrained: "openai"
    image_size: 224
    hidden_size: 512
    
  fusion_head:
    input_dim: 1280  # 768 (text) + 512 (image)
    hidden_dims: [512, 256, 128]
    output_dim: 1
    dropout: 0.1
    activation: "gelu"
    
features:
  text_embedding: true
  image_embedding: true
  cross_modal_similarity: true
  image_similarity: true
  delta_features: false
  
training:
  batch_size: 32
  learning_rate: 0.0001
  weight_decay: 0.01
  epochs: 10
  warmup_steps: 1000
  max_steps: 10000
  gradient_clip_norm: 1.0
  seeds: [42, 43, 44]
  
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 0.00000001
    
  scheduler:
    type: "linear"
    warmup_ratio: 0.1
    
  data:
    train_split: 0.8
    val_split: 0.1
    test_split: 0.1
    shuffle: true
    num_workers: 4
    
evaluation:
  metrics: ["auc", "f1", "accuracy", "precision", "recall", "latency_p50", "latency_p95", "throughput", "model_size"]
  platforms: ["cpu", "cuda", "mps"]
  batch_sizes: [1, 4, 8, 16]
  
  calibration:
    method: "platt_scaling"
    validation_split: 0.2
    
performance:
  target_latency_p95_ms: 100
  target_throughput_img_per_sec: 10
  max_model_size_mb: 500
  
hardware:
  gpu_memory_gb: 8
  cpu_cores: 4
  memory_gb: 16
  
output:
  save_model: true
  save_checkpoints: true
  export_onnx: true
  export_coreml: false
  
logging:
  level: "INFO"
  save_logs: true
  tensorboard: true
