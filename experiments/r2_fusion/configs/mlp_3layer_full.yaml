# R2: Fusion Architecture Ablations
# 3-Layer MLP with Full Features and Calibration

experiment:
  name: "r2_mlp_3layer_full"
  description: "3-layer MLP fusion head with full features and isotonic calibration"
  type: "r2_fusion"
  
models:
  text_encoder:
    name: "bert-base-uncased"
    type: "bert"
    max_length: 512
    hidden_size: 768
    pooling: "mean"
    
  image_encoder:
    name: "open_clip/ViT-B-32"
    type: "clip"
    pretrained: "laion2b_s34b_b79k"
    image_size: 224
    hidden_size: 512
    
  fusion_head:
    type: "mlp"
    input_dim: 1792  # 768 (text) + 512 (image) + 256 (similarities) + 256 (deltas)
    hidden_dims: [1024, 512, 256]
    output_dim: 1
    dropout: 0.1
    activation: "gelu"
    
features:
  text_embedding: true
  image_embedding: true
  cross_modal_similarity: true
  image_similarity: true
  delta_features: true
  structural_features: true
  
training:
  batch_size: 24  # Smaller batch due to larger feature dimension
  learning_rate: 1e-4
  weight_decay: 0.01
  epochs: 12  # More epochs for complex model
  warmup_steps: 1000
  max_steps: 12000
  gradient_clip_norm: 1.0
  seeds: [42, 43, 44]
  
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1e-8
    
  scheduler:
    type: "linear"
    warmup_ratio: 0.1
    
  data:
    train_split: 0.8
    val_split: 0.1
    test_split: 0.1
    shuffle: true
    num_workers: 4
    
evaluation:
  metrics: ["auc", "f1", "accuracy", "precision", "recall", "ece", "brier_score"]
  platforms: ["cpu", "cuda", "mps"]
  
  calibration:
    method: "isotonic_regression"
    validation_split: 0.2
    
performance:
  target_latency_p95_ms: 80
  target_throughput_img_per_sec: 15
  max_model_size_mb: 200
  
hardware:
  gpu_memory_gb: 8
  cpu_cores: 4
  memory_gb: 16
  
output:
  save_model: true
  save_checkpoints: true
  export_onnx: true
  export_coreml: false
  
logging:
  level: "INFO"
  save_logs: true
  tensorboard: true
